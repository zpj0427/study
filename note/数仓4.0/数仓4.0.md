# 1，用户行为采集平台

## 1.1，数仓概述

### 1.1.1，数仓基本概念

> 数据仓库（ Data Warehouse ）：<font color=red>是为企业制定决策，提供数据支持的</font>。可以帮助企业，改进业务流程、提高产品质量等
>
> 数据仓库的输入数据通常包括：**业务数据**、**用户行为数据**、**爬虫数据**等

* **业务数据**：就是各行业在处理事务过程中产生的数据。比如用户在电商网站中登录、下单、支付等过程中，需要和网站后台数据库进行增删改查交互，产生的数据就是业务数据。业务数据通常存储在 `MySQL`、`Oracle` 等数据库中；

* **用户行为数据**：用户在使用产品过程中，<font color=red>通过埋点收集与客户端产品交互</font>过程中产生的数据，并发往日志服务器进行保存。比如页面浏览、点击、停留、评论、点赞、收藏等。用户行为数据通常存储在日志文件中；

* **爬虫数据**：通常事通过技术手段获取其他公司网站的数据；<font color=red>爬虫爬的好、局里坐到老</font>

  ![1653983886213](E:\gitrepository\study\note\images\数仓4.0\1653983886213.png)

### 1.1.2，项目需求

> 整个用户行为采集平台由多个模块组成，具体如下：

1. <font color=red>用户行为数据采集平台</font>搭建
2. <font color=red>业务数据采集平台</font>搭建
3. <font color=red>数据仓库维度搭建</font>
4. 分析，<font color=red>设备、会员、商品、地区、活动</font>等电商核心主题，统计的报表指标近100个
5. 采用<font color=red>即席查询工具</font>，随时进行指标分析
6. 对<font color=red>集群性能进行监控</font>，发生异常需要报警
7. <font color=red>元数据管理</font>
8. <font color=red>质量监控</font>
9. <font color=red>权限管理</font>

### 1.1.3，技术选型

> 技术选型需要从数据处理的各个节点进行考虑；
>
> 技术选型的主要考虑因素：数据量大小、业务需求、行业内经验、技术成熟度、开发维护成本、总成本预算

1. 数据采集传输：<font color=red>`Flume`、`Kafka`、`Sqoop`</font>、`Logstash`、`DataX`
2. 数据存储：<font color=red>`MySQL`、`HDFS`、`HBase`</font>、`Redis`、`MongoDB`
3. 数据计算：<font color=red>`Hive`、`Tez`、`Spark`</font>、`Flink`、`Storm`
4. 数据查询：<font color=red>`Presto`、`Kylin`</font>、`Impala`、`Druid`、`ClickHouse`、`Doris`
5. 数据可视化：<font color=red>`Echarts`、`Superset`</font>、`QuickBI`、`DataV`
6. 任务调度：<font color=red>`Azkaban`、</font>`Oozie`、`DolphinScheduler`、`Airflow`
7. 集群监控：<font color=red>`Zabbix`</font>、`Prometheus`
8. 元数据管理：<font color=red>`Atlas`</font>
9. 权限管理：<font color=red>`Ranger`</font>、`Sentry`

### 1.1.4，系统数据设计

![1653983974278](E:\gitrepository\study\note\images\数仓4.0\1653983974278.png)

### 1.1.5，框架版本选型

1. 框架选型

   * `Apache`：<font color=red>运维麻烦，组件间的兼容性需要自行调研</font>（一般大厂使用）（建议使用）（本次使用）
   * `CDH`：国内使用最大的版本，不开源，目前已经开始收费（1W$/节点/年）（不建议使用）
   * `HDP`：开源，可进行二次开发，但是没有 `CDH` 稳定，国内使用较少

2. 云服务选型

   * 阿里云：`EMR`、`MaxCompute`、`DataWorks`
   * 亚马逊云：`EMR`
   * 腾讯云：`EMR`
   * 华为云：`EMR`

3. 组件版本选型

   | **框架**    | **旧版本** | **新版本** |
   | ----------- | ---------- | ---------- |
   | Hadoop      | 2.7.2      | 3.1.3      |
   | Zookeeper   | 3.4.10     | 3.5.7      |
   | MySQL       | 5.6.24     | 5.7.16     |
   | Hive        | 1.2.1      | 3.1.2      |
   | Flume       | 1.7.0      | 1.9.0      |
   | Kafka       | 0.11-0.2   | 2.4.1      |
   | Kafka Eagle | 1.3.7      | 1.4.5      |
   | Azkaban     | 2.5.0      | 3.84.4     |
   | Spark       | 2.1.1      | 3.0.0      |
   | Hbase       | 1.3.1      | 2.0.5      |
   | Phoenix     | 4.14.1     | 5.0.0      |
   | Sqoop       | 1.4.6      | 1.4.6      |
   | Presto      | 0.189      | 0.189      |
   | Kylin       | 2.5.1      | 3.0.1      |
   | Atlas       | 0.8.4      | 2.0.0      |
   | Ranger      | 2.0.0      | 2.0.0      |
   | Solr        | 5.2.1      | 7.7.0      |

### 1.1.6，集群规模

#### 1.1.6.1，集群规模计算

1. 如果每日日活100万人，每人产生100条数据，则每日会产生 <font color=red>1亿条数据</font>
2. 每条数据1KB左右，则每日数据量为：<font color=red>100GB</font>
3. 半年内不扩容、不清除，则累积数据为：<font color=red>18TB</font>
4. 数据保存三个副本：<font color=red>54TB</font>
5. 服务器预留30%左右的空间：<font color=red>77TB</font>
6. 此外，需要考虑数据分层处理、数据压缩（ES=75%，Hive=25%）、关系型数据库、消息队列等

#### 1.1.6.2，生产集群规划

* 消耗内存较大组件分开，如 `nn` 和 `rm`
* 数据传输较紧密需要放在一起，如 `kafka` 和 `zookeeper`
* 客户端尽量放在同一台机器上，方便外部访问，如 `mysql`、`hive`
* 有依赖关系的尽量放在同一台机器，如 `hive` 和 `Azkaban Executor`

| 1       | 2       | 3     | 4     | 5     | 6    | 7    | 8     | 9     | 10    |
| ------- | ------- | ----- | ----- | ----- | ---- | ---- | ----- | ----- | ----- |
| nn      | nn      | dn    | dn    | dn    | dn   | dn   | dn    | dn    | dn    |
|         |         | rm    | rm    | nm    | nm   | nm   | nm    | nm    | nm    |
|         |         | nm    | nm    |       |      |      |       |       |       |
|         |         |       |       |       |      |      | zk    | zk    | zk    |
|         |         |       |       |       |      |      | kafka | kafka | kafka |
|         |         |       |       |       |      |      | Flume | Flume | flume |
|         |         | Hbase | Hbase | Hbase |      |      |       |       |       |
| hive    | hive    |       |       |       |      |      |       |       |       |
| mysql   | mysql   |       |       |       |      |      |       |       |       |
| spark   | spark   |       |       |       |      |      |       |       |       |
| Azkaban | Azkaban |       |       |       | ES   | ES   |       |       |       |

#### 1.1.6.3，测试集群规划

| 服务名称              | 子服务           | 服务器hadoop102 | 服务器hadoop103 | 服务器hadoop104 |
| --------------------- | ---------------- | --------------- | --------------- | --------------- |
| HDFS                  | NameNode         | √               |                 |                 |
| DataNode              | √                | √               | √               |                 |
| SecondaryNameNode     |                  |                 | √               |                 |
| Yarn                  | NodeManager      | √               | √               | √               |
| Resourcemanager       |                  | √               |                 |                 |
| Zookeeper             | Zookeeper Server | √               | √               | √               |
| Flume（采集日志）     | Flume            | √               | √               |                 |
| Kafka                 | Kafka            | √               | √               | √               |
| Flume（消费Kafka）    | Flume            |                 |                 | √               |
| Hive                  | Hive             | √               |                 |                 |
| MySQL                 | MySQL            | √               |                 |                 |
| Sqoop                 | Sqoop            | √               |                 |                 |
| Presto                | Coordinator      | √               |                 |                 |
| Worker                |                  | √               | √               |                 |
| Azkaban               | AzkabanWebServer | √               |                 |                 |
| AzkabanExecutorServer | √                |                 |                 |                 |
| Spark                 |                  | √               |                 |                 |
| Kylin                 |                  | √               |                 |                 |
| HBase                 | HMaster          | √               |                 |                 |
| HRegionServer         | √                | √               | √               |                 |
| Superset              |                  | √               |                 |                 |
| Atlas                 |                  | √               |                 |                 |
| Solr                  | Jar              | √               |                 |                 |
| 服务数总计            |                  | 19              | 8               | 8               |

## 1.2，数据生成模块

### 1.2.1，目标数据

> 在业务过程中，需要搜集和分析的数据只要包括：<font color=red>页面数据、事件数据、曝光数据、启动数据、错误数据</font>

#### 1.2.1.1，页面数据

#### 1.2.1.2，事件数据

#### 1.2.1.3，曝光数据

#### 1.2.1.4，启动数据

#### 1.2.1.5，错误数据

### 1.2.2，数据埋点

#### 1.2.2.1，主流埋点方式

> 目前主流的埋点方式：包括<font color=red>代码埋点、可视化埋点、全埋点</font>

* **代码埋点**：通过调用埋点 `SDK` 函数，在需要埋点的业务逻辑位置调用接口，上报锚点数据；比如，通过对某个按钮的点击时间添加埋点函数调用，在点击该按钮时，会调用内部的 `SDK` 函数，发送数据
* **可视化埋点**：只需要研发人员集成采集 `SDK`，不需要写埋点代码，业务人员就可以通过访问分析平台的“圈选”功能，来“圈”出需要对用户行为进行捕捉的控件，并对该事件进行命名。圈选完毕后，这些配置会同步到各个用户的终端上，由采集 `SDK` 按照圈选的配置自动进行用户行为数据的采集和发送
* **全埋点**：通过在产品中嵌入 `SDK`，前端自动采集页面上的全部用户行为事件，上报埋点数据，相当于做了一个统一的埋点；然后再通过界面配置哪些数据需要在业务中进行分析

#### 1.2.2.2，埋点数据上报时机

* 在离开页面时，上传这个页面产生的所有数据（行为、事件、曝光、错误等）：
  * 优点：批处理，减少服务器交互，减轻压力；缺点：非实时
* 每个行为、事件、曝光、错误产生时，立即发送：
  * 优点：响应及时；缺点：对服务器压力较大
* 本次数据采集采用<font color=red>非实施方式</font>

#### 1.2.2.3，埋点数据日志结构

```json
{
	"common": {}, // 公共参数
	"actions": {}, // 行为数据
	"displays": {}, // 曝光数据
	"page": {}, // 页面数据
	"err": {}, // 错误数据
	"ts":  // 时间戳
}
```

### 1.2.3，服务器和JDK准备

> 参考文件 `../大数据环境搭建/环境搭建` 的 `2、基础环境准备`，进行模板机搭建和虚拟机克隆
>
> 配置模板机、克隆三台机器、准备分发脚本、配置SSH免密登陆
>
> 三台机器：Hadoop102、Hadoop103、Hadoop104
>
> JDK版本：`jdk-8u212-linux-x64.tar.gz`

#### 1.2.3.1，环境变量加载

* `Linux` 的环境变量在多个文件中配置，包括：`/etc/profile`，`/etc/prifile.d/*.sh`，`~/.bashrc`，`~/.bash_profile`

* `bash` 的运行模式可分为 `login shell` 和 `non-login shell`：`login shell` 表示通过终端，输入用户名密码连接之后；`non-login shell` 是指通过 `ssh` 等方式直接执行命令；

* `login shell` 和 `non-login shell` 的主要区别在于启动时会加载不同的配置文件：<font color=red>`login shell` 启动时会加载 `/etc/profile`、`~/.base_profile`、`~/.bashrc`；`non-login shell` 启动时只会加载 `~/.bashrc`</font>

  ![1653985504202](E:\gitrepository\study\note\images\数仓4.0\1653985504202.png)

* 在 `~/.bashrc` 文件中，会向下进行文件调用，由 `/etc/bashrc` 到 `/etc/profile.d/*.sh` 文件，所以：<font color=red>无论是 `login shell` 还是 `non-login shell` 都会加载 `/etc/profile.d/*.sh` 中的环境变量</font>

### 1.2.3，模拟数据

#### 1.2.3.1，使用日志

* 在 `./模拟数据/日志` 下上传四个文件到 `/opt/mockdata` 路径下

  ![1654075005418](E:\gitrepository\study\note\images\数仓4.0\1654075005418.png) 

* 配置文件：

  * `application.yaml` ：对 `mock.data` 参数进行修改，表示业务日期
  * `path.json`：根据需求，灵活配置用户点路径和比例
  * `logback.xml`：配置日志生成路径

* 生成日志：

  * 通过 `java -jar` 执行 `jar` 包，最终日志文件会生成在 `/opt/mockdata/log/` 文件夹下
  
    ```sh
    # 注意：一定要在指定目录下执行jar包
    [root@Hadoop102 mockdata]# java -jar gmall2020-mock-log-2021-01-22.jar 
    ```
  

![1654692087190](E:\gitrepository\study\note\images\数仓4.0\1654692087190.png)

#### 1.2.3.2，集群日志生成脚本

* 在 `/root/bin` 目录下创建文件 `log.sh`

  ```sh
  [root@Hadoop102 log]# vim /root/bin/log.sh
  ## 标准输入0: 从键盘输入
  ## 标准输出1: 
  #!/bin/bash
  for i in hadoop102 hadoop103; do
      echo "========== $i =========="
      ssh $i "cd /opt/mockdata/; java -jar gmall2020-mock-log-2021-01-22.jar >/dev/null 2>&1 &"
  done 
  [root@Hadoop102 log]# chmod 777 /root/bin/log.sh
  ```



## 1.3，数据采集模块

### 1.3.1，`Hadoop` 安装

> `Hadoop` 集群环境安装参考 `../大数据环境搭建/环境搭建`

#### 1.3.1.1，项目经验之 `HDFS` 多目录存储

* 在生产环境中，由于硬盘不足，[需要添加一块磁盘](https://www.cnblogs.com/yujianadu/p/10750698.html)，添加磁盘后，新加磁盘需要加入到 `HDFS` 的写目录中
* 在 `hdfs-site.xml` 文件中配置多目录，<font color=red>注意新加磁盘的读写访问权限</font>

* `HDFS` 的 `DataNode` 节点保存数据的路径是由 `dfs.datanode.data.dir` 参数决定，其默认值为 `file://${hadoop.tmp.dir}/dfs/data`，若服务器存在多块盘，则必须对该参数进行修改，比如改成：

  ```xml
  <property>
      <name>dfs.datanode.data.dir</name>
  <value>file:///dfs/data1,file:///sdb/dfs/data3,file:///sdc/dfs/data4</value>
  </property>
  ```

#### 1.3.1.2，集群数据均衡

> 集群数据均衡包括 节点间数据均衡 和 磁盘间数据均衡

1. 节点间数据均衡

   * 开启数据均衡命令

     ```sh
     # 10表示集群中各个节点磁盘利用率相差不超过10%，可根据实际进行调整
     [root@Hadoop102 hadoop]# start-balancer.sh -threshold 10
     ```

   * 停止数据均衡命令

     ```sh
     [root@Hadoop102 hadoop]# stop-balancer.sh
     ```

2. 磁盘间数据均衡

   * 生成均衡计划（机器只有一块盘，不会生成均衡计划）

     ```sh
     # 磁盘使用率低于10%不会生成均衡计划
     # 只有一块盘不会生成均衡计划
     [root@Hadoop102 hadoop]# hdfs diskbalancer -plan hadoop102
     ```

   * 执行均衡计划

     ```sh
     [root@Hadoop102 hadoop]# hdfs diskbalancer -execute hadoop102.plan.json
     ```

   * 查看当前均衡任务的执行情况

     ```sh
     [root@Hadoop102 hadoop]# hdfs diskbalancer -query hadoop102
     ```

   * 取消均衡任务

     ```sh
     [root@Hadoop102 hadoop]# hdfs diskbalancer -cancel hadoop102.plan.json
     ```

#### 1.3.1.3，项目经验之 `LZO` 压缩配置

1. `hadoop-lzo` 编译

   * `hadoop` 本身并不支持 `LZO` 压缩，需要使用 `twitter` 提供的 `hadoop-lzo` 开源组件，`hadoop-lzo` 编译需要依赖 `hadoop` 和 `lzo` 组件，编译步骤参考：`./hadoop-lzo/hadoop-lzo编译.txt`
   * 成品包为：`./haddoop-lzo/hadoop-lzo-0.4.20.jar`

2. 将编译好的 `hadoop-lzo-xxx.jar` 包上传并同步到其他节点

   ```sh
   # 上传成品包到 `${HADOOP_HOME}/share/hadoop/common` 目录下
   [root@Hadoop102 common]# ls | grep lzo
   hadoop-lzo-0.4.20.jar
   [root@Hadoop102 common]# pwd
   /opt/hadoop-3.1.3/share/hadoop/common
   ```

3. `core-site.xml` 增加配置支持 `LZO` 压缩

   ```xml
   <!-- ${HAODOP_HOME}/etc/hadoop/core-site.xml -->
   <property>
   	<name>io.compression.codecs</name>
   	<value>
   		org.apache.hadoop.io.compress.GzipCodec,
   		org.apache.hadoop.io.compress.DefaultCodec,
   		org.apache.hadoop.io.compress.BZip2Codec,
   		org.apache.hadoop.io.compress.SnappyCodec,
   		com.hadoop.compression.lzo.LzoCodec,
   		com.hadoop.compression.lzo.LzopCodec
   	</value>
   </property>
   <property>
   	<name>io.compression.codec.lzo.class</name>
   	<value>com.hadoop.compression.lzo.LzoCodec</value>
   </property>
   ```

4. 同步 `jar` 包和 `core-site.xml` 文件到其他节点

   ```sh
   # 同步jar包
   [root@Hadoop102 hadoop]# xsync /opt/hadoop-3.1.3/share/hadoop/common/
   # 同步配置文件
   [root@Hadoop102 hadoop]# xsync /opt/hadoop-3.1.3/etc/hadoop/core-site.xml
   ```

5. 重启 `Hadoop` 集群

   ```sh
   # 关闭 hadoop
   [root@Hadoop102 hadoop]# myhadoop stop
   # 启动 hadoop
   [root@Hadoop102 hadoop]# myhadoop start 
   ```

6. 测试-数据准备

   ```sh
   # 添加文件夹
   [root@Hadoop102 hadoop-3.1.3]# hadoop fs -mkdir /input
   # 随便上传一个文件
   [root@Hadoop102 hadoop-3.1.3]# hadoop fs -put README.txt /input
   # 数据结果, 可以看到文件大小为 1366B
   [root@Hadoop102 hadoop-3.1.3]# hadoop fs -ls /input
   Found 1 items
   -rw-r--r--   3 root supergroup       1366 2022-06-25 23:28 /input/README.txt
   ```

7. 测试-压缩

   ```sh
   # LZO压缩处理
   [root@Hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.output.fileoutputformat.compress=true -Dmapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec  /input /output-lzo
   # LZO压缩-结果, 最终文件大小为1037B
   [root@Hadoop102 hadoop-3.1.3]# hadoop fs -ls /output-lzo/
   Found 2 items
   -rw-r--r--   3 root supergroup          0 2022-06-25 23:35 /output-lzo/_SUCCESS
   -rw-r--r--   3 root supergroup       1037 2022-06-25 23:35 /output-lzo/part-r-00000.lzo
   
   # 常规压缩
   [root@Hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount  /input /output
   # 常规压缩-结果, 最终文件大小为 1306B
   [root@Hadoop102 hadoop-3.1.3]# hadoop fs -ls /output/
   Found 2 items
   -rw-r--r--   3 root supergroup          0 2022-06-25 23:36 /output/_SUCCESS
   -rw-r--r--   3 root supergroup       1306 2022-06-25 23:36 /output/part-r-00000
   ```

#### 1.3.1.4，项目经验值 `LZO` 创建索引

> `LZO` 压缩文件的可切片特性依赖其索引，因此需要手动对 `LZO` 压缩文件创建索引，如果没有索引，则 `LZO` 文件的切片只有一个

1. 数据文件准备

   ```sh
   # 上传一个大小为200M+的lzo文件
   [root@Hadoop102 hadoop-3.1.3]# hadoop fs -mkdir /input-lzo
   # 上传文件
   [root@Hadoop102 hadoop-3.1.3]# hadoop fs -put bigtable.lzo /input-lzo
   # 查看HDFS文件
   [root@Hadoop102 hadoop-3.1.3]# hadoop fs -ls /input-lzo/
   Found 1 items
   -rw-r--r--   3 root supergroup  224565455 2022-06-25 23:45 /input-lzo/bigtable.lzo
   ```

2. 无索引测试

   ```sh
   # 执行 `wordcount` 程序
   [root@Hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input-lzo /output-noindex
   ```

   ![1656226463778](E:\gitrepository\study\note\images\数仓4.0\1656226463778.png)

3. 有索引测试

   * 创建索引

     ```sh
     [root@Hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/common/hadoop-lzo-0.4.20.jar  com.hadoop.compression.lzo.DistributedLzoIndexer /input-lzo/bigtable.lzo
     ```

   * 执行计算测试

     ```sh
     [root@Hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input-lzo /output-index
     ```

     ![1656226943548](E:\gitrepository\study\note\images\数仓4.0\1656226943548.png)

4. 常见问题处理

   * 执行过程中，如果出现如下字样，需要修改 `yarn-site.xml` 文件

     ```sh
     [2022-06-25 23:47:47.397]Container [pid=57488,containerID=container_1656223501309_0005_01_000004] is running 268888576B beyond the 'VIRTUAL' memory limit. Current usage: 199.9 MB of 1 GB physical memory used; 2.4 GB of 2.1 GB virtual memory used. Killing container.
     ```

   * 修改 `yarn-site.xml` 文件，并分发到其他节点

     ```xml
     <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
     <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
     </property>
     ```

   * 修改完成后，重启 `hadoop` 集群

#### 1.3.1.5，项目经验之基准测试

1. 基准测试环境准备
   * `HDFS` 的读写性能受网络和磁盘的影响比较大，为了方便测试，将 `hadoop` 三台节点的虚拟机网络设置为 `100mbps`（`mbps` 单位是 `bit`, `10M/s` 单位是 `byte`, 1byte=8bit, `100mbps` = `12.5M/s`）

     ![1656226909734](E:\gitrepository\study\note\images\数仓4.0\1656226909734.png)

   * 网速测试：通过 `python` 创建一个链接，直接通过页面访问 `http://192.168.10.102:8000`
   
     ```sh
     [root@Hadoop102 hadoop-3.1.3]# python -m SimpleHTTPServer
     ```
   
   * 下载刚才上传的 `bigdata.lzo` 文件，可以看到下载速度在 `10M/s` 左右徘徊

2. `HDFS` 写性能测试

   * 写性能测试底层原理

     ![1656227612623](E:\gitrepository\study\note\images\数仓4.0\1656227612623.png)

   * 写性能测试

     ```sh
     # 3个128M文件，各有3个副本
     [root@Hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 3 -fileSize 128MB
     ```

     ```sh
     2022-06-26 00:23:07,063 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write
     2022-06-26 00:23:07,063 INFO fs.TestDFSIO:             Date & time: Sun Jun 26 00:23:07 PDT 2022
     # 生成 `MapTask` 数量，一般是（CPU核数 - 1）
     2022-06-26 00:23:07,063 INFO fs.TestDFSIO:         Number of files: 3
     # 每个 `map` 处理的文件大小
     2022-06-26 00:23:07,063 INFO fs.TestDFSIO:  Total MBytes processed: 384
     # *****单个 `MapTask` 的吞吐量 = 总文件大小 / 每一个 `MapTask` 写数据的时间累加；集群整体吞吐量 = 生成 `MapTask` 数量 * 单个 `MapTask` 吞吐量
     2022-06-26 00:23:07,063 INFO fs.TestDFSIO:       Throughput mb/sec: 3.86
     # *****平均 `MapTask` 吞吐量 = 每个 `MapTask` 处理文件大小 / 每个 `MapTask` 处理文件时长，全部相加后 / `Task` 数量
     2022-06-26 00:23:07,063 INFO fs.TestDFSIO:  Average IO rate mb/sec: 3.94
     # 方差，反应各个 `MapTask` 的差值，越小越均衡
     2022-06-26 00:23:07,063 INFO fs.TestDFSIO:   IO rate std deviation: 0.61
     2022-06-26 00:23:07,063 INFO fs.TestDFSIO:      Test exec time sec: 72.16
     ```

   * 测试结果分析

     ![1656228179033](E:\gitrepository\study\note\images\数仓4.0\1656228179033.png)

     * 副本1在 本地，不参与网络传输，只对副本2和副本3的网络传输进行考虑计算
     * 一共参与测试的文件副本 = 3 * 2 = 6个副本
     * 压测后速度为：`3.86M/s`，实测速度：`3.86M/s` * 6 = `22M/s`
     * 三台服务器的带宽：12.5 + 12.5 + 12.5 ≈ 30M/s
     * <font color=red>所以电脑该换了</font>

3. `HDFS` 读性能测试

   * 测试读取

     ```sh
     [root@Hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 3 -fileSize 128MB
     ```

     ```sh
     2022-06-26 00:34:06,880 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read
     2022-06-26 00:34:06,880 INFO fs.TestDFSIO:             Date & time: Sun Jun 26 00:34:06 PDT 2022
     2022-06-26 00:34:06,880 INFO fs.TestDFSIO:         Number of files: 3
     2022-06-26 00:34:06,880 INFO fs.TestDFSIO:  Total MBytes processed: 384
     2022-06-26 00:34:06,880 INFO fs.TestDFSIO:       Throughput mb/sec: 91.28
     2022-06-26 00:34:06,880 INFO fs.TestDFSIO:  Average IO rate mb/sec: 91.65
     2022-06-26 00:34:06,880 INFO fs.TestDFSIO:   IO rate std deviation: 5.84
     2022-06-26 00:34:06,880 INFO fs.TestDFSIO:      Test exec time sec: 38.53
     2022-06-26 00:34:06,880 INFO fs.TestDFSIO:
     ```

   * 删除临时数据

     ```sh
     [root@Hadoop102 hadoop-3.1.3]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean
     ```

### 1.3.2，`Zookeeper` 安装

* `Zookeeper` 安装，参考 `../大数据环境搭建/环境搭建.md` 下的 `6，Zookeeper 集群搭建`

  ```sh
  [root@Hadoop102 opt]# myzookeeper status
  =============== 状态: hadoop102 ===============
  ZooKeeper JMX enabled by default
  Using config: /opt/zookeeper-3.5.7/bin/../conf/zoo.cfg
  Client port found: 2181. Client address: localhost.
  Mode: follower
  =============== 状态: hadoop103 ===============
  ZooKeeper JMX enabled by default
  Using config: /opt/zookeeper-3.5.7/bin/../conf/zoo.cfg
  Client port found: 2181. Client address: localhost.
  Mode: leader
  =============== 状态: hadoop104 ===============
  ZooKeeper JMX enabled by default
  Using config: /opt/zookeeper-3.5.7/bin/../conf/zoo.cfg
  Client port found: 2181. Client address: localhost.
  Mode: follower
  ```

### 1.3.3，`Kafka` 安装

* `Kafka` 安装，参考 `../大数据环境搭建/环境搭建.md` 下的 `8，Kafka 集群搭建`

#### 1.3.3.1，`Kafka` 压力测试

1. `Kafka` 压测

   * 用 `Kafka` 自带的脚本，可以完成对 `Kafka` 的压测
   * 生产者压测：`kafka-producer-perf-test.sh`
   * 消费者压测：`kafka-consumer-perf-test.sh`
   * `Kafka` 压测，是在磁盘读写速度一定的情况下，可以查看哪些部分出现了性能瓶颈（CPU、内存、网络IO），一般都是网络IO达到瓶颈

2. `Kafka Producer` 压力测试

   ```sh
   # record-size：一条消息的大小，单位是字节
   # num-records：发送消息数
   # throughput：每秒消息条数，-1表示不限流
   # batch.size：批处理条数，以每批为单位进行发送，不够一批等待
   # linger.ms：延迟等待时间，对 `batch.size` 的补充，不够一批但到了延迟等待时间，也会发送
   [root@Hadoop102 kafka-2.11]# bin/kafka-producer-perf-test.sh  --topic test_1 --record-size 100 --num-records 10000000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=500 linger.ms=5
   ```

3. `Kafka Consumer` 压力测试

   ```sh
   # fetch-size：每次fetch的数据大小
   # messages：总共需要消费的消息个数
   [root@Hadoop102 kafka-2.11]# bin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic test --fetch-size 10000 --messages 10000000 --threads 1
   ```

### 1.3.4，采集日志 `Flume`

#### 1.3.4.1，`Flume` 安装

* 安装流程参考：`../大数据环境搭建/环境搭建.md` 下的 `9，Flume 安装`，这部分安装未完成，需要继续往下走
* 在这一步，`Flume` 只部署在 `hadoop102` 和 `hadoop103` 节点上

#### 1.3.4.2，`Flume` 组件选型

1. `Source`

   * `Exec Source`：支持从文件中实时读取数据；如果服务宕机，二次读取不会读取未消费的历史数据
   * `Spooling Directory Source`：监控目录，可支持断电续传；不能支持实时读取
   * `Taildir Source`：可实时读取，支持断点续传；<font color=red>采用该 `Source` 类型</font>

2. `Channel`

   * `File Channel`：数据存储在文件中，可靠性高，性能低
   * `Memory Channel`：数据存储在内存中，可靠性低，性能高
   * `Kafka Channel`：数据存储在 `Kafka` 中，存储在磁盘，可靠性高，性能高，同事省去了 `Sink`

   * `Flume 1.7` 之前，`Kafka Channel` 的 `parseAsFlumeEvent` 设置不生效，使用较少，本次使用 `Flume 1.9`，<font color=red>采用 `Kafka Channel`</font>

#### 1.3.4.3，日志采集 `Flume` 配置

1. `Flume` 配置分析

   * 从组件选型中，可以对 `Flume` 日志采集的整套流程进行分析
   * 数据从日志文件中来，通过 `TailDir Source` 进行读取
   * `Source` 读取完成后，通过 `LogInterceptor` 进行日志清洗，过滤掉错误日志
   * `Interceptor` 过滤完日志后，将日志通过 `Kafka Channel` 发出
   * 最终将数据发到 `Kafka` 的指定 `topic`，无 `Sink` 参与
   * 所以，整体路径是：`File` -> `TailDir Source` -> `LogInterceptor` -> `Kafka Channel` -> `topic`

   ![1656336517385](E:\gitrepository\study\note\images\数仓4.0\1656336517385.png)

2. `Flume` 具体配置：可以通过上述流程，对 `Flume` 进行配置，并将各个组件连接

   * 在 `${FLUME_HOME/conf}` 目录下创建 `file_flume_kafka.conf` 文件

     ```sh
     # file: 表示从文件来
     # flume: 表示在flume服务中处理
     # kafka: 表示最终输出到kafka中去
     [root@Hadoop102 conf]# vim file_flume_kafka.conf
     ```

     ```properties
     # a1: Flume服务名称
     a1.sources = r1 # Source组件名称
     a1.channels = c1 # Channel组件名称
     
     #描述source
     # Flume服务名称.组件类型.组件名称.参数 = 值
     a1.sources.r1.type = TAILDIR # 使用 TailDir Source 组件
     a1.sources.r1.filegroups = f1 # 定义文件组, 表示文件组名称
     a1.sources.r1.filegroups.f1 = /opt/mockdata/log/app.* # 文件组信息
     a1.sources.r1.positionFile = /opt/flume-1.9.0/taildir_position.json # TailDir Source 需要一个文件记录文件读取偏移量, 就是这个文件
     a1.sources.r1.interceptors =  i1 # 拦截器名称
     a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.ETLInterceptor$Builder # 自定义拦截器, 这里需要自己打jar包
     
     #描述channel
     a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel # Kafka Channel组件
     a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092 # Kafka集群, 写两台机器就够用了
     a1.channels.c1.kafka.topic = flume_collect_log # 发往的kafka.topic
     a1.channels.c1.parseAsFlumeEvent = false # 不以Flume的形式格式化数据
     
     #绑定source和channel以及sink和channel的关系
     a1.sources.r1.channels = c1 # 将Source和Channle连接起来
     # 此处无 Sink
     ```

#### 1.3.4.4，`Flume` 拦截器

1. 创建 `Maven` 功能：`flume-interceptor`

2. 在服务中创建包路径：`com.atguigu.flume.interceptor`

3. 在 `pom` 文件中添加如下配置

   ```xml
   <dependencies>
   	<dependency>
   		<groupId>org.apache.flume</groupId>
   		<artifactId>flume-ng-core</artifactId>
   		<version>1.9.0</version>
   		<!-- 只编译, 不打包 -->
   		<scope>provided</scope>
   	</dependency>
   
   	<dependency>
   		<groupId>com.alibaba</groupId>
   		<artifactId>fastjson</artifactId>
   		<version>1.2.62</version>
   	</dependency>
   </dependencies>
   
   <build>
   	<plugins>
   		<plugin>
   			<artifactId>maven-compiler-plugin</artifactId>
   			<version>2.3.2</version>
   			<configuration>
   				<source>1.8</source>
   				<target>1.8</target>
   			</configuration>
   		</plugin>
   		<plugin>
   			<artifactId>maven-assembly-plugin</artifactId>
   			<configuration>
   				<descriptorRefs>
   					<descriptorRef>jar-with-dependencies</descriptorRef>
   				</descriptorRefs>
   			</configuration>
   			<executions>
   				<execution>
   					<id>make-assembly</id>
   					<phase>package</phase>
   					<goals>
   						<goal>single</goal>
   					</goals>
   				</execution>
   			</executions>
   		</plugin>
   	</plugins>
   </build>
   ```

4. 在包路径下添加拦截器类：`ETLInterceptor`，并实现接口 `org.apache.flume.interceptor.Interceptor`

   ```java
   package com.atguigu.flume.interceptor;
   
   import com.alibaba.fastjson.JSON;
   import org.apache.flume.Context;
   import org.apache.flume.Event;
   import org.apache.flume.interceptor.Interceptor;
   
   import java.nio.charset.Charset;
   import java.util.Iterator;
   import java.util.List;
   
   // 自定义拦截器
   public class ETLInterceptor implements Interceptor {
       @Override
       public void initialize() { }
   
       @Override
       public Event intercept(Event event) {
           byte[] body = event.getBody();
           String message = new String(body, Charset.forName("UTF-8"));
           try {
               // 用JSON进行解析, 如果报错了说明不对
               JSON.parse(message);
               return event;
           } catch (Exception e) {
               return null;
           }
       }
   
       @Override
       public List<Event> intercept(List<Event> list) {
           Iterator<Event> iterator = list.iterator();
           for (;iterator.hasNext();) {
               Event event = iterator.next();
               // 遍历每一条数据进行处理, 调用单数据接口, 如果返回null, 说明不对, 直接移除
               if (null == intercept(event))
                   iterator.remove();
           }
           return list;
       }
   
       @Override
       public void close() { }
   
       // 此外, 需要自定义Builder类, 实现 Interceptor.Builder, 完成拦截器构造
       public static class Builder implements Interceptor.Builder {
   
           @Override
           public Interceptor build() {
               return new ETLInterceptor();
           }
   
           @Override
           public void configure(Context context) { }
       }
   }
   ```

5. 打包，通过 `Maven` 进行 `clean`、`install`，最终输出包：`flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar`

6. 上传该 `jar` 包到 `${FLUME_HOME}/lib` 路径下

   ```sh
   [root@Hadoop102 lib]# ls | grep inter
   flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar
   [root@Hadoop102 lib]# pwd
   /opt/flume-1.9.0/lib
   ```

7. 分发 `${FLUME_HOME}` 到 `hadoop103` 、`hadoop104` 节点

   ```sh
   [root@Hadoop102 ~]# xsync /opt/flume-1.9.0/
   ```

8. `Flume` 启停命令，一键启停脚本；启动 `Flume` 前，需要保证 `Kafka` 为启动状态

   ```sh
   #! /bin/bash
   
   case $1 in
   "start"){
   	for i in hadoop102 hadoop103
   	do
   		echo " --------启动 $i 采集flume-------"
   		ssh $i "nohup /opt/flume-1.9.0/bin/flume-ng agent --conf-file /opt/flume-1.9.0/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/flume-1.9.0/logs/log1.txt 2>&1  &"
   	done
   };;	
   "stop"){
   	for i in hadoop102 hadoop103
   	do
   		echo " --------停止 $i 采集flume-------"
   		# awk  '{print \$2}'：表示按空格分割，取第二个参数，这里取的是PID
   		# xargs -n1：以行为单位前面分割的值，-n1表示每次取一行，取到的值参与后面的命令执行
   		ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
   	done
   };;
   esac
   ```

#### 1.3.4.5，测试 `Flume` - `Kafka` 通道

1. 之前已经执行过模拟数据，在 `/opt/mockdata/log` 下已经存在日志数据文件，在启动 `Flume` 前，可以复制 `hadoop102` 会话，创建一个消费者消费 `flume_collect_log`；服务启动后，日志滚动即为正常

   ```sh
   [root@Hadoop102 bin]# ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic flume_collect_log
   ```

2. 启动滚动完成后，不需要关闭监听，继续模拟数据，测试持续读取是否正常；执行完 `jar` 包后，会生成数据文件，可以看到 `Kafka` 监听页面持续消费数据，即为正常

   ```sh
   [root@Hadoop102 mockdata]# java -jar gmall2020-mock-log-2021-01-22.jar 
   ```

3. 如果没有数据消费，说明 `Flume` 启动或者数据处理错误，可查看日志文件 `/opt/flume-1.9.0/logs/log1.txt`，即启动脚本中定义的日志文件路径

### 1.3.5，消费数据 `Flume`



